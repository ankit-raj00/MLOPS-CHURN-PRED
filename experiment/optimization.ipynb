{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "bebdf422",
            "metadata": {},
            "source": [
                "# Model Optimization: SMOTE & Optuna (Bayesian Tuning)\n",
                "\n",
                "In this notebook, we aim to maximize **F1-Score** by:\n",
                "1.  **Handling Imbalance**: Applying **SMOTE** (Synthetic Minority Over-sampling Technique).\n",
                "2.  **Advanced Tuning**: Using **Optuna** for Bayesian Hyperparameter Optimization of XGBoost."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "a9234c58",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "d:\\MLOPS PROJECT CHURN PRED\\customer_venc\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:178: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance. For migrating existing data, https://github.com/mlflow/mlflow-export-import can be used.\n",
                        "  return FileStore(store_uri, store_uri)\n",
                        "2026/01/06 16:31:36 INFO mlflow.tracking.fluent: Experiment with name 'Churn_Prediction_Optimization' does not exist. Creating a new experiment.\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<Experiment: artifact_location='file:d:/MLOPS PROJECT CHURN PRED/experiment/../mlruns/221989432216233886', creation_time=1767697296536, experiment_id='221989432216233886', last_update_time=1767697296536, lifecycle_stage='active', name='Churn_Prediction_Optimization', tags={}>"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import mlflow\n",
                "import mlflow.sklearn\n",
                "import optuna\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from xgboost import XGBClassifier\n",
                "from imblearn.over_sampling import SMOTE\n",
                "from imblearn.pipeline import Pipeline as ImbPipeline\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
                "\n",
                "%matplotlib inline\n",
                "\n",
                "# Set MLflow Tracking URI\n",
                "mlflow.set_tracking_uri(\"file:../mlruns\")\n",
                "mlflow.set_experiment(\"Churn_Prediction_Optimization\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6a0c8742",
            "metadata": {},
            "source": [
                "## 1. Load Data & Preprocessing (Same as Baseline)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "139e3062",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Data Processed. Train Shape: (12800, 16)\n"
                    ]
                }
            ],
            "source": [
                "df = pd.read_csv('../customer_churn_dataset/train.csv')\n",
                "X = df.drop('churn', axis=1)\n",
                "y = df['churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
                "\n",
                "# Split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "\n",
                "# --- Reuse Preprocessing Logic ---\n",
                "def impute_internet_service(X_data, knn_model=None, scaler=None, is_train=True):\n",
                "    X = X_data.copy()\n",
                "    impute_features = ['monthly_charges', 'total_charges', 'tenure']\n",
                "    if is_train:\n",
                "        scaler = StandardScaler()\n",
                "        scaler.fit(X[impute_features])\n",
                "    X_scaled = scaler.transform(X[impute_features])\n",
                "    mask_missing = X['internet_service'].isnull()\n",
                "    if is_train:\n",
                "        X_train_knn = X_scaled[~mask_missing]\n",
                "        y_train_knn = X.loc[~mask_missing, 'internet_service']\n",
                "        knn_model = KNeighborsClassifier(n_neighbors=5)\n",
                "        knn_model.fit(X_train_knn, y_train_knn)\n",
                "    if mask_missing.sum() > 0:\n",
                "        X_missing_knn = X_scaled[mask_missing]\n",
                "        imputed_values = knn_model.predict(X_missing_knn)\n",
                "        X.loc[mask_missing, 'internet_service'] = imputed_values\n",
                "    return X, knn_model, scaler\n",
                "\n",
                "# 1. Impute\n",
                "X_train_imp, knn_imputer, knn_scaler = impute_internet_service(X_train, is_train=True)\n",
                "X_test_imp, _, _ = impute_internet_service(X_test, knn_model=knn_imputer, scaler=knn_scaler, is_train=False)\n",
                "\n",
                "# 2. Encode & Scale\n",
                "numerical_cols = ['tenure', 'monthly_charges', 'total_charges']\n",
                "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
                "if 'customer_id' in categorical_cols: categorical_cols.remove('customer_id')\n",
                "\n",
                "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
                "scaler_final = StandardScaler()\n",
                "\n",
                "X_train_enc = ohe.fit_transform(X_train_imp[categorical_cols])\n",
                "X_test_enc = ohe.transform(X_test_imp[categorical_cols])\n",
                "X_train_sc = scaler_final.fit_transform(X_train_imp[numerical_cols])\n",
                "X_test_sc = scaler_final.transform(X_test_imp[numerical_cols])\n",
                "\n",
                "X_train_final = np.hstack([X_train_sc, X_train_enc])\n",
                "X_test_final = np.hstack([X_test_sc, X_test_enc])\n",
                "\n",
                "print(\"Data Processed. Train Shape:\", X_train_final.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4db5552b",
            "metadata": {},
            "source": [
                "## 2. Define Optuna Objective Function\n",
                "We want to find params that give the **Best F1 Score** on Cross-Validation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "6664c52e",
            "metadata": {},
            "outputs": [],
            "source": [
                "def objective(trial):\n",
                "    # 1. Propose Hyperparameters\n",
                "    params = {\n",
                "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
                "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
                "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
                "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
                "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
                "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
                "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
                "        'eval_metric': 'logloss',\n",
                "        'n_jobs': -1\n",
                "    }\n",
                "    \n",
                "    # 2. Build Pipeline with SMOTE + XGBoost\n",
                "    # We use ImbPipeline to ensure SMOTE is only applied to the TRAIN fold during CV\n",
                "    model = XGBClassifier(**params)\n",
                "    \n",
                "    pipeline = ImbPipeline([\n",
                "        ('smote', SMOTE(random_state=42)),\n",
                "        ('model', model)\n",
                "    ])\n",
                "    \n",
                "    # 3. Cross-Validation (3-Fold to save time, use 5 for more robustness)\n",
                "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
                "    scores = cross_val_score(pipeline, X_train_final, y_train, cv=cv, scoring='f1', n_jobs=-1)\n",
                "    \n",
                "    return scores.mean()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f88c3bec",
            "metadata": {},
            "source": [
                "## 3. Run Optuna Optimization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "79d4a39e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[I 2026-01-06 16:31:36,777] A new study created in memory with name: XGB_Imbalanced_Optimization\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting Optuna Optimization...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[I 2026-01-06 16:31:43,848] Trial 0 finished with value: 0.6062382765289511 and parameters: {'n_estimators': 326, 'learning_rate': 0.1831080288279283, 'max_depth': 8, 'subsample': 0.9861469714312541, 'colsample_bytree': 0.8846580310428939, 'gamma': 2.6240457449311623, 'min_child_weight': 3}. Best is trial 0 with value: 0.6062382765289511.\n",
                        "[I 2026-01-06 16:31:50,459] Trial 1 finished with value: 0.5981530316582121 and parameters: {'n_estimators': 394, 'learning_rate': 0.29234709207556403, 'max_depth': 10, 'subsample': 0.5559897550811289, 'colsample_bytree': 0.7025151970779693, 'gamma': 2.5787844250369347, 'min_child_weight': 8}. Best is trial 0 with value: 0.6062382765289511.\n",
                        "[I 2026-01-06 16:31:55,376] Trial 2 finished with value: 0.6071243117909534 and parameters: {'n_estimators': 296, 'learning_rate': 0.21483319448157145, 'max_depth': 6, 'subsample': 0.8231898156331081, 'colsample_bytree': 0.6732275508840055, 'gamma': 4.990604069677028, 'min_child_weight': 4}. Best is trial 2 with value: 0.6071243117909534.\n",
                        "[I 2026-01-06 16:31:56,492] Trial 3 finished with value: 0.6049686706533226 and parameters: {'n_estimators': 152, 'learning_rate': 0.048400025530956896, 'max_depth': 9, 'subsample': 0.7849355442078702, 'colsample_bytree': 0.616718817081557, 'gamma': 0.7304718804709637, 'min_child_weight': 3}. Best is trial 2 with value: 0.6071243117909534.\n",
                        "[I 2026-01-06 16:31:58,674] Trial 4 finished with value: 0.6030473391737905 and parameters: {'n_estimators': 351, 'learning_rate': 0.07595181800945895, 'max_depth': 5, 'subsample': 0.855426305699382, 'colsample_bytree': 0.8213811138329359, 'gamma': 0.5474608863616426, 'min_child_weight': 3}. Best is trial 2 with value: 0.6071243117909534.\n",
                        "[I 2026-01-06 16:31:59,770] Trial 5 finished with value: 0.6099170284981524 and parameters: {'n_estimators': 363, 'learning_rate': 0.17467105748992173, 'max_depth': 5, 'subsample': 0.9948704623324767, 'colsample_bytree': 0.7289105251236538, 'gamma': 1.350169274517921, 'min_child_weight': 7}. Best is trial 5 with value: 0.6099170284981524.\n",
                        "[I 2026-01-06 16:32:01,344] Trial 6 finished with value: 0.6092622948231181 and parameters: {'n_estimators': 482, 'learning_rate': 0.05694016209017862, 'max_depth': 5, 'subsample': 0.8647996979562209, 'colsample_bytree': 0.9930336598060603, 'gamma': 1.9644699339059417, 'min_child_weight': 2}. Best is trial 5 with value: 0.6099170284981524.\n",
                        "[I 2026-01-06 16:32:02,426] Trial 7 finished with value: 0.6081753981679254 and parameters: {'n_estimators': 316, 'learning_rate': 0.04523502788585541, 'max_depth': 5, 'subsample': 0.6607723782049779, 'colsample_bytree': 0.9856328242878785, 'gamma': 2.110851995187386, 'min_child_weight': 1}. Best is trial 5 with value: 0.6099170284981524.\n",
                        "[I 2026-01-06 16:32:03,058] Trial 8 finished with value: 0.6132354054371815 and parameters: {'n_estimators': 154, 'learning_rate': 0.13693357019100774, 'max_depth': 4, 'subsample': 0.96601593175214, 'colsample_bytree': 0.8613139733454416, 'gamma': 2.556094525951737, 'min_child_weight': 4}. Best is trial 8 with value: 0.6132354054371815.\n",
                        "[I 2026-01-06 16:32:04,745] Trial 9 finished with value: 0.608672287538815 and parameters: {'n_estimators': 427, 'learning_rate': 0.01903238090777486, 'max_depth': 8, 'subsample': 0.6774772452393198, 'colsample_bytree': 0.9982573605062328, 'gamma': 2.213195498428582, 'min_child_weight': 1}. Best is trial 8 with value: 0.6132354054371815.\n",
                        "[I 2026-01-06 16:32:05,273] Trial 10 finished with value: 0.6082413359683542 and parameters: {'n_estimators': 109, 'learning_rate': 0.1053069002020034, 'max_depth': 3, 'subsample': 0.9090168520371935, 'colsample_bytree': 0.5399771746567505, 'gamma': 3.6789951051241063, 'min_child_weight': 6}. Best is trial 8 with value: 0.6132354054371815.\n",
                        "[I 2026-01-06 16:32:05,826] Trial 11 finished with value: 0.6088120572054233 and parameters: {'n_estimators': 219, 'learning_rate': 0.12119653934735422, 'max_depth': 3, 'subsample': 0.9904059015772284, 'colsample_bytree': 0.7971495347388966, 'gamma': 1.1878110709361214, 'min_child_weight': 10}. Best is trial 8 with value: 0.6132354054371815.\n",
                        "[I 2026-01-06 16:32:06,417] Trial 12 finished with value: 0.6105677576264116 and parameters: {'n_estimators': 237, 'learning_rate': 0.15902522331854813, 'max_depth': 4, 'subsample': 0.9296990423391196, 'colsample_bytree': 0.8775412227606023, 'gamma': 3.4733056878300914, 'min_child_weight': 6}. Best is trial 8 with value: 0.6132354054371815.\n",
                        "[I 2026-01-06 16:32:07,066] Trial 13 finished with value: 0.6067620378848666 and parameters: {'n_estimators': 226, 'learning_rate': 0.02404685166946421, 'max_depth': 3, 'subsample': 0.9166182727035488, 'colsample_bytree': 0.8700300458592182, 'gamma': 3.53233141522611, 'min_child_weight': 5}. Best is trial 8 with value: 0.6132354054371815.\n",
                        "[I 2026-01-06 16:32:07,828] Trial 14 finished with value: 0.6060561155552144 and parameters: {'n_estimators': 208, 'learning_rate': 0.010825036729810994, 'max_depth': 4, 'subsample': 0.7201097355158765, 'colsample_bytree': 0.9079157894619228, 'gamma': 3.5284801416108977, 'min_child_weight': 5}. Best is trial 8 with value: 0.6132354054371815.\n",
                        "[I 2026-01-06 16:32:08,434] Trial 15 finished with value: 0.6074399401964522 and parameters: {'n_estimators': 262, 'learning_rate': 0.11127182357901687, 'max_depth': 7, 'subsample': 0.9214835200854304, 'colsample_bytree': 0.7882640049124385, 'gamma': 4.334641100174641, 'min_child_weight': 8}. Best is trial 8 with value: 0.6132354054371815.\n",
                        "[I 2026-01-06 16:32:08,927] Trial 16 finished with value: 0.6035983299980056 and parameters: {'n_estimators': 159, 'learning_rate': 0.28144440593127956, 'max_depth': 4, 'subsample': 0.5313747058796869, 'colsample_bytree': 0.9228918553000879, 'gamma': 3.2315661756886165, 'min_child_weight': 6}. Best is trial 8 with value: 0.6132354054371815.\n",
                        "[I 2026-01-06 16:32:09,335] Trial 17 finished with value: 0.604104350659587 and parameters: {'n_estimators': 101, 'learning_rate': 0.13807573808336462, 'max_depth': 6, 'subsample': 0.7687758092234651, 'colsample_bytree': 0.8340084515373034, 'gamma': 3.0447318852298353, 'min_child_weight': 9}. Best is trial 8 with value: 0.6132354054371815.\n",
                        "[I 2026-01-06 16:32:09,810] Trial 18 finished with value: 0.6058373370942359 and parameters: {'n_estimators': 177, 'learning_rate': 0.06649172702359592, 'max_depth': 4, 'subsample': 0.9402028638942838, 'colsample_bytree': 0.9313507217437486, 'gamma': 4.251512337369893, 'min_child_weight': 4}. Best is trial 8 with value: 0.6132354054371815.\n",
                        "[I 2026-01-06 16:32:10,552] Trial 19 finished with value: 0.6070906281639955 and parameters: {'n_estimators': 271, 'learning_rate': 0.08699490798481395, 'max_depth': 7, 'subsample': 0.8607368972573155, 'colsample_bytree': 0.7619132329260246, 'gamma': 1.5282693799552685, 'min_child_weight': 7}. Best is trial 8 with value: 0.6132354054371815.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Best F1: 0.6132354054371815\n",
                        "Best Params: {'n_estimators': 154, 'learning_rate': 0.13693357019100774, 'max_depth': 4, 'subsample': 0.96601593175214, 'colsample_bytree': 0.8613139733454416, 'gamma': 2.556094525951737, 'min_child_weight': 4}\n"
                    ]
                }
            ],
            "source": [
                "# Create Study\n",
                "study = optuna.create_study(direction=\"maximize\", study_name=\"XGB_Imbalanced_Optimization\")\n",
                "\n",
                "# Run for 20 Trials\n",
                "print(\"Starting Optuna Optimization...\")\n",
                "study.optimize(objective, n_trials=20)\n",
                "\n",
                "print(\"Best F1:\", study.best_value)\n",
                "print(\"Best Params:\", study.best_params)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b7201596",
            "metadata": {},
            "source": [
                "## 4. Train & Log Best Model\n",
                "Now we take the best params found by Optuna, retrain on the FULL train set (with SMOTE), and evaluate on Test."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "20c95a49",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Final Evaluation (Optuna Best) ---\n",
                        "Accuracy: 0.7688\n",
                        "F1 Score: 0.5916\n",
                        "AUC-ROC: 0.7522\n",
                        "\n",
                        "Classification Report:\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.77      0.92      0.84      2102\n",
                        "           1       0.75      0.49      0.59      1098\n",
                        "\n",
                        "    accuracy                           0.77      3200\n",
                        "   macro avg       0.76      0.70      0.72      3200\n",
                        "weighted avg       0.77      0.77      0.75      3200\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "best_params = study.best_params\n",
                "best_params['eval_metric'] = 'logloss'\n",
                "\n",
                "with mlflow.start_run(run_name=\"XGBoost_Optuna_SMOTE\"):\n",
                "    # Log Params\n",
                "    mlflow.log_params(best_params)\n",
                "    mlflow.log_param(\"optimization\", \"Optuna\")\n",
                "    \n",
                "    # Apply SMOTE to Full Train Set one last time before final training\n",
                "    smote = SMOTE(random_state=42)\n",
                "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_final, y_train)\n",
                "    \n",
                "    # Train Final Model\n",
                "    best_model = XGBClassifier(**best_params)\n",
                "    best_model.fit(X_train_resampled, y_train_resampled)\n",
                "    \n",
                "    # Evaluate on Test\n",
                "    y_pred = best_model.predict(X_test_final)\n",
                "    y_prob = best_model.predict_proba(X_test_final)[:, 1]\n",
                "    \n",
                "    acc = accuracy_score(y_test, y_pred)\n",
                "    f1 = f1_score(y_test, y_pred)\n",
                "    roc = roc_auc_score(y_test, y_prob)\n",
                "    \n",
                "    # Log Metrics\n",
                "    mlflow.log_metric(\"accuracy\", acc)\n",
                "    mlflow.log_metric(\"f1_score\", f1)\n",
                "    mlflow.log_metric(\"auc_roc\", roc)\n",
                "    \n",
                "    mlflow.sklearn.log_model(best_model, name=\"best_xgb_optuna\")\n",
                "    \n",
                "    print(\"\\n--- Final Evaluation (Optuna Best) ---\")\n",
                "    print(f\"Accuracy: {acc:.4f}\")\n",
                "    print(f\"F1 Score: {f1:.4f}\")\n",
                "    print(f\"AUC-ROC: {roc:.4f}\")\n",
                "    print(\"\\nClassification Report:\")\n",
                "    print(classification_report(y_test, y_pred))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "customer_venc",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

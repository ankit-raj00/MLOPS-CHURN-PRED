{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Advanced Model Comparison\n",
                "\n",
                "In this notebook, we experiment with models known for handling imbalance and categorical data effectively:\n",
                "1.  **Random Forest (Balanced)**: Using native `class_weight='balanced'` parameter.\n",
                "2.  **LightGBM**: Highly efficient gradient boosting, using `class_weight='balanced'`.\n",
                "3.  **CatBoost**: Handles categorical features natively and has robust `auto_class_weights`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<Experiment: artifact_location='file:d:/MLOPS PROJECT CHURN PRED/experiment/../mlruns/982143950750977900', creation_time=1767698036854, experiment_id='982143950750977900', last_update_time=1767698036854, lifecycle_stage='active', name='Churn_Prediction_Advanced_Models', tags={'mlflow.experimentKind': 'custom_model_development'}>"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import mlflow\n",
                "import mlflow.sklearn\n",
                "import mlflow.lightgbm\n",
                "import mlflow.catboost\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from lightgbm import LGBMClassifier\n",
                "from catboost import CatBoostClassifier\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
                "\n",
                "%matplotlib inline\n",
                "\n",
                "# Set MLflow Tracking URI\n",
                "mlflow.set_tracking_uri(\"file:../mlruns\")\n",
                "mlflow.set_experiment(\"Churn_Prediction_Advanced_Models\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data & Preprocessing (Same as Baseline)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Data Processed. Train Shape: (16000, 16)\n"
                    ]
                }
            ],
            "source": [
                "df = pd.read_csv('../customer_churn_dataset/customer_churn_dataset.csv')\n",
                "X = df.drop('churn', axis=1)\n",
                "y = df['churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
                "\n",
                "# Split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "\n",
                "# --- Reuse Preprocessing Logic ---\n",
                "def impute_internet_service(X_data, knn_model=None, scaler=None, is_train=True):\n",
                "    X = X_data.copy()\n",
                "    impute_features = ['monthly_charges', 'total_charges', 'tenure']\n",
                "    if is_train:\n",
                "        scaler = StandardScaler()\n",
                "        scaler.fit(X[impute_features])\n",
                "    X_scaled = scaler.transform(X[impute_features])\n",
                "    mask_missing = X['internet_service'].isnull()\n",
                "    if is_train:\n",
                "        X_train_knn = X_scaled[~mask_missing]\n",
                "        y_train_knn = X.loc[~mask_missing, 'internet_service']\n",
                "        knn_model = KNeighborsClassifier(n_neighbors=5)\n",
                "        knn_model.fit(X_train_knn, y_train_knn)\n",
                "    if mask_missing.sum() > 0:\n",
                "        X_missing_knn = X_scaled[mask_missing]\n",
                "        imputed_values = knn_model.predict(X_missing_knn)\n",
                "        X.loc[mask_missing, 'internet_service'] = imputed_values\n",
                "    return X, knn_model, scaler\n",
                "\n",
                "# 1. Impute\n",
                "X_train_imp, knn_imputer, knn_scaler = impute_internet_service(X_train, is_train=True)\n",
                "X_test_imp, _, _ = impute_internet_service(X_test, knn_model=knn_imputer, scaler=knn_scaler, is_train=False)\n",
                "\n",
                "# 2. Encode & Scale\n",
                "numerical_cols = ['tenure', 'monthly_charges', 'total_charges']\n",
                "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
                "if 'customer_id' in categorical_cols: categorical_cols.remove('customer_id')\n",
                "\n",
                "# For CatBoost, we usually keep strings, but for fair comparison we'll use OHE first (or we can use CatBoost native... let's stick to consistent OHE for now to compare algorithm power on SAME data)\n",
                "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
                "scaler_final = StandardScaler()\n",
                "\n",
                "X_train_enc = ohe.fit_transform(X_train_imp[categorical_cols])\n",
                "X_test_enc = ohe.transform(X_test_imp[categorical_cols])\n",
                "X_train_sc = scaler_final.fit_transform(X_train_imp[numerical_cols])\n",
                "X_test_sc = scaler_final.transform(X_test_imp[numerical_cols])\n",
                "\n",
                "X_train_final = np.hstack([X_train_sc, X_train_enc])\n",
                "X_test_final = np.hstack([X_test_sc, X_test_enc])\n",
                "\n",
                "print(\"Data Processed. Train Shape:\", X_train_final.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define Training & Tracking Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_and_log(model, name):\n",
                "    with mlflow.start_run(run_name=name):\n",
                "        # Log Params\n",
                "        mlflow.log_params(model.get_params())\n",
                "        \n",
                "        # Train\n",
                "        print(f\"Training {name}...\")\n",
                "        model.fit(X_train_final, y_train)\n",
                "        \n",
                "        # Predict\n",
                "        y_pred = model.predict(X_test_final)\n",
                "        y_prob = model.predict_proba(X_test_final)[:, 1] if hasattr(model, 'predict_proba') else None\n",
                "        \n",
                "        # Metrics\n",
                "        acc = accuracy_score(y_test, y_pred)\n",
                "        f1 = f1_score(y_test, y_pred)\n",
                "        roc = roc_auc_score(y_test, y_prob) if y_prob is not None else 0\n",
                "        \n",
                "        # Log Metrics\n",
                "        mlflow.log_metric(\"accuracy\", acc)\n",
                "        mlflow.log_metric(\"f1_score\", f1)\n",
                "        mlflow.log_metric(\"auc_roc\", roc)\n",
                "        \n",
                "        # Log Model\n",
                "        if \"LightGBM\" in name:\n",
                "            mlflow.lightgbm.log_model(model, name=name)\n",
                "        elif \"CatBoost\" in name:\n",
                "            mlflow.catboost.log_model(model, name=name)\n",
                "        else:\n",
                "            mlflow.sklearn.log_model(model, name=name)\n",
                "        \n",
                "        print(f\"Finished {name}: Acc={acc:.4f}, F1={f1:.4f}\")\n",
                "        print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Run Experiments"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Random_Forest_Balanced...\n",
                        "Finished Random_Forest_Balanced: Acc=0.7408, F1=0.5630\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.77      0.87      0.82      2631\n",
                        "           1       0.67      0.49      0.56      1369\n",
                        "\n",
                        "    accuracy                           0.74      4000\n",
                        "   macro avg       0.72      0.68      0.69      4000\n",
                        "weighted avg       0.73      0.74      0.73      4000\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# 1. Random Forest (Balanced)\n",
                "rf_balanced = RandomForestClassifier()\n",
                "train_and_log(rf_balanced, \"Random_Forest_Balanced\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training LightGBM_Balanced...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "d:\\MLOPS PROJECT CHURN PRED\\customer_venc\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
                        "  warnings.warn(\n",
                        "d:\\MLOPS PROJECT CHURN PRED\\customer_venc\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Finished LightGBM_Balanced: Acc=0.7485, F1=0.5970\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.78      0.85      0.82      2631\n",
                        "           1       0.66      0.54      0.60      1369\n",
                        "\n",
                        "    accuracy                           0.75      4000\n",
                        "   macro avg       0.72      0.70      0.71      4000\n",
                        "weighted avg       0.74      0.75      0.74      4000\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# 2. LightGBM (Balanced)\n",
                "lgbm_balanced = LGBMClassifier(\n",
                "    n_estimators=200,\n",
                "    learning_rate=0.1,\n",
                "    class_weight='balanced',  # <--- Key Change\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    verbosity=-1\n",
                ")\n",
                "train_and_log(lgbm_balanced, \"LightGBM_Balanced\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training CatBoost_Balanced...\n",
                        "Finished CatBoost_Balanced: Acc=0.7492, F1=0.5957\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.78      0.86      0.82      2631\n",
                        "           1       0.66      0.54      0.60      1369\n",
                        "\n",
                        "    accuracy                           0.75      4000\n",
                        "   macro avg       0.72      0.70      0.71      4000\n",
                        "weighted avg       0.74      0.75      0.74      4000\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# 3. CatBoost (Balanced)\n",
                "cat_balanced = CatBoostClassifier(\n",
                "    iterations=200,\n",
                "    learning_rate=0.1,\n",
                "    depth=6,\n",
                "    auto_class_weights='Balanced', # <--- Key Change\n",
                "    random_seed=42,\n",
                "    verbose=0  # Silent training\n",
                ")\n",
                "train_and_log(cat_balanced, \"CatBoost_Balanced\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "customer_venc",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
